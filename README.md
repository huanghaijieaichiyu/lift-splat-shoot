# Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D

PyTorch code for Lift-Splat-Shoot (ECCV 2020).

**Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D**  
Jonah Philion, [Sanja Fidler](http://www.cs.toronto.edu/~fidler/)\
ECCV, 2020 (Poster)\
**[[Paper](https://arxiv.org/abs/2008.05711)] [[Project Page](https://nv-tlabs.github.io/lift-splat-shoot/)] [[10-min video](https://youtu.be/oL5ISk6BnDE)] [[1-min video](https://youtu.be/ypQQUG4nFJY)]**

**Abstract:**
The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single "bird's-eye-view" coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a bird's-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to "lift" each image individually into a frustum of features for each camera, then "splat" all frustums into a rasterized bird's-eye-view grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird's-eye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by "shooting" template trajectories into a bird's-eye-view cost map output by our network. We benchmark our approach against models that use oracle depth from lidar. Project page: [https://nv-tlabs.github.io/lift-splat-shoot/](https://nv-tlabs.github.io/lift-splat-shoot/).

**Questions/Requests:** Please file an [issue](https://github.com/nv-tlabs/lift-splat-shoot/issues) if you have any questions or requests about the code or the [paper](https://arxiv.org/abs/2008.05711). If you prefer your question to be private, you can alternatively email me at jphilion@nvidia.com.

### Citation
If you found this codebase useful in your research, please consider citing
```
@inproceedings{philion2020lift,
    title={Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D},
    author={Jonah Philion and Sanja Fidler},
    booktitle={Proceedings of the European Conference on Computer Vision},
    year={2020},
}
```

### Preparation
Download nuscenes data from [https://www.nuscenes.org/](https://www.nuscenes.org/). Install dependencies.

```
pip install nuscenes-devkit tensorboardX efficientnet_pytorch==0.7.0
```

### Pre-trained Model
Download a pre-trained BEV vehicle segmentation model from here: [https://drive.google.com/file/d/18fy-6beTFTZx5SrYLs9Xk7cY-fGSm7kw/view?usp=sharing](https://drive.google.com/file/d/18fy-6beTFTZx5SrYLs9Xk7cY-fGSm7kw/view?usp=sharing)

| Vehicle IOU (reported in paper)        | Vehicle IOU (this repository)         |
|:-------------:|:-------------:| 
| 32.07      | 33.03 |

### Evaluate a model
Evaluate the IOU of a model on the nuScenes validation set. To evaluate on the "mini" split, pass `mini`. To evaluate on the "trainval" split, pass `trainval`.

```
python main.py eval_model_iou mini --modelf=runs/train/best.pt --dataroot=../datasets
```

### Visualize Predictions
Visualize the BEV segmentation output by a model:

```
python main.py viz_model_preds mini --modelf=runs/train/best.pt --dataroot=../datasets --map_folder=../datasets/mini
```
<img src="./imgs/eval.gif">

### Visualize Input/Output Data (optional)
Run a visual check to make sure extrinsics/intrinsics are being parsed correctly. Left: input images with LiDAR scans projected using the extrinsics and intrinsics. Middle: the LiDAR scan that is projected. Right: X-Y projection of the point cloud generated by the lift-splat model. Pass `--viz_train=True` to view data augmentation.

```
python main.py lidar_check mini/trainval --dataroot=NUSCENES_ROOT --viz_train=False
```
<img src="./imgs/check.gif">

### Train a model (optional)
Train a model. Monitor with tensorboard.

```
python main.py train mini/trainval --dataroot=NUSCENES_ROOT --logdir=./runs --gpuid=0
tensorboard --logdir=./runs --bind_all
```

### Acknowledgements
Thank you to Sanja Fidler, as well as David Acuna, Daiqing Li, Amlan Kar, Jun Gao, Kevin, Xie, Karan Sapra, the NVIDIA AV Team, and NVIDIA Research for their help in making this research possible.

# Lift-Splat-Shoot with BEVENet for 3D Object Detection

This repo contains the PyTorch implementation of the methods described in:

1. [Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D](https://arxiv.org/abs/2008.05711) (ECCV 2020 Oral)
2. [Towards Efficient 3D Object Detection in Bird's-Eye-View Space for Autonomous Driving: A Convolutional-Only Approach](https://arxiv.org/abs/2312.00633)

## News

- **[2024-x-x]** We added support for BEVENet, a convolutional-only approach for 3D object detection that is 3x faster than ViT-based methods!

## Installation

Install all the dependencies
```
conda create --name lss python=3.7
conda activate lss
pip install -r requirements.txt
```

## Data Preparation

Download the nuScenes dataset from [https://www.nuscenes.org/](https://www.nuscenes.org/) and then update the `dataroot` argument in the example command below.

## Usage

### Semantic Segmentation (Original LSS method)

```
python main.py train \
        --dataroot="/data/nuscenes" \
        --nepochs=10000 \
        --gpuid=0 \
        --H=900 \
        --W=1600 \
        --train_step=5 \
        --bsz=4
```

### 3D Object Detection (BEVENet method)

Our implementation of BEVENet enables efficient 3D object detection in Bird's-Eye-View using a fully convolutional approach, avoiding the quadratic complexity of ViT-based architectures.

Train a BEVENet model for 3D object detection:

```
python main.py train_3d_detection \
        --dataroot="/data/nuscenes" \
        --nepochs=10000 \
        --gpuid=0 \
        --H=900 \
        --W=1600 \
        --bsz=4 \
        --num_classes=10
```

Evaluate the 3D object detection performance:

```
python main.py eval_3d_detection \
        --dataroot="/data/nuscenes" \
        --checkpoint_path="./runs_3d_detection/exp_000/best.pt" \
        --gpuid=0
```

## BEVENet Architecture

BEVENet follows the lift-splat-shoot paradigm but replaces vision transformer components with efficient convolutional operations:

1. **Image Encoder**: Uses RepVit blocks for efficient feature extraction
2. **Bird's-Eye-View Encoder**: Utilizes convolutional blocks for efficient BEV feature representation
3. **Detection Head**: Multi-task head that predicts object classes, 3D boxes, and confidence scores

## Comparison with ViT-based Methods

| Method | mAP | FPS | Parameters |
|--------|-----|-----|------------|
| BEVFormer | 0.416 | 2.0 | 78M |
| BEVDet | 0.349 | 13.5 | 40M |
| **BEVENet (Ours)** | 0.456 | 47.6 | 36M |

## References

If you find this code useful, please cite:
```
@inproceedings{philion2020lift,
  title={Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D},
  author={Philion, Jonah and Fidler, Sanja},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  year={2020}
}

@article{li2023towards,
  title={Towards Efficient 3D Object Detection in Bird's-Eye-View Space for Autonomous Driving: A Convolutional-Only Approach},
  author={Li, Yuxin and Han, Qiang and Yu, Mengying and Jiang, Yuxin and Yeo, Chaikiat and Li, Yiheng and Huang, Zihang and Liu, Nini and Chen, Hsuanhan and Wu, Xiaojun},
  journal={arXiv preprint arXiv:2312.00633},
  year={2023}
}
```
